Tinygrad is a minimal, lightweight deep learning framework designed with simplicity, performance, and portability in mind. Itâ€™s written in Python and emphasizes a clean, readable codebaseâ€”often cited as one of its key strengths. Hereâ€™s what you need to know, factually and without hype:

Core Design Philosophy
Minimalism: Tinygrad aims to be as small as possible while still supporting essential deep learning operations. The core codebase is under 1,000 lines of Python (excluding tests and utilities), making it highly accessible for learning and auditing.
No external dependencies: It avoids heavy dependencies like NumPy or PyTorch, instead relying on pure Python and a minimal set of C extensions (e.g., for GPU kernels via CUDA or OpenCL).
Runtime compilation: It uses a just-in-time (JIT) compiler approach via its own TinyIR (intermediate representation), which allows for optimizations like kernel fusion and loop unrolling.
Key Features
GPU support: Tinygrad supports CUDA and OpenCL for GPU acceleration. It compiles kernels at runtime using a custom compiler stack, which can be efficient for small to medium-sized models.
Autograd: Implements automatic differentiation via a computational graph, similar to PyTorchâ€™s eager mode. Itâ€™s functional and works well for training small models.
Model support: Can run models like ResNet-18, MobileNet, and even some transformer variants (e.g., TinyBERT), though itâ€™s not optimized for large-scale production use.
Cross-platform: Designed to run on CPUs, GPUs (via CUDA/OpenCL), and even on embedded devices (e.g., Raspberry Pi) due to its low footprint.
Performance
Competitive for small models: On small models and simple tasks, Tinygrad can match or exceed PyTorch in speed due to aggressive kernel fusion and low overhead.
Not optimized for large-scale training: It lacks distributed training, mixed precision (beyond basic FP16), and advanced memory management features found in mature frameworks.
Compilation overhead: JIT compilation can introduce latency on first run, which may be problematic in latency-sensitive applications.
Limitations
Limited ecosystem: No built-in support for data loading pipelines, model zoo, or deployment tools (e.g., TorchScript, ONNX).
No native support for dynamic shapes: While it handles dynamic control flow, itâ€™s not as robust as PyTorch or TensorFlow in handling variable-length sequences or complex control flow.
Community and documentation: Smaller community compared to PyTorch or TensorFlow. Documentation is improving but still sparse in some areas.

---

# âœ… PHASE 1: FOUNDATION â€“ DISTRIBUTED TRAINING ABSTRACTION  
## (PRs 1â€“3)  
**Goal**: Build a minimal, composable, dependency-free distributed runtime that enables:
- Multi-GPU (single node)
- Multi-node (via TCP/UCX)
- `torchrun`-like orchestration
- Core primitives: `all_reduce`, `all_gather`, `broadcast`, `send`, `recv`
- Determinism via `DistributedSampler`
- Activation recomputation via `checkpoint`
- ZeRO-2 via `ZeroOptimizer`

Weâ€™ll walk through **each PR in full technical depth**, then show how to **execute them from the terminal**.

---

## ðŸ”¹ PR #1: `tinygrad.distributed` â€“ Minimal Distributed Runtime

### ðŸŽ¯ **Objective**
Implement a **lightweight, dependency-free distributed runtime** that:
- Initializes process groups (rank/world size)
- Provides low-level communication primitives
- Uses **MPI or UCX** (optional) â€” **no PyTorch**
- Supports **non-blocking** operations
- Is **composable**, **not monolithic**

---

### ðŸ§± **Implementation Details**

#### 1. **Directory Structure**
```bash
tinygrad/
â”œâ”€â”€ distributed/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ process_group.py
â”‚   â”œâ”€â”€ communication.py
â”‚   â””â”€â”€ backend/
â”‚       â”œâ”€â”€ mpi.py
â”‚       â””â”€â”€ ucx.py
```

> âœ… **No `torch` dependency** â€” all communication is self-implemented.

---

#### 2. **`process_group.py` â€“ Process Group Management**

```python
# tinygrad/distributed/process_group.py
import os
from typing import Optional

# Global state (minimal, not global in the sense of "global variable")
_rank = None
_world_size = None
_backend = None

def init_process_group(backend: str = "mpi", **kwargs):
    global _rank, _world_size, _backend
    if _rank is not None:
        raise RuntimeError("Process group already initialized")
    
    # Detect environment
    _rank = int(os.getenv("RANK", 0))
    _world_size = int(os.getenv("WORLD_SIZE", 1))
    _backend = backend

    # Validate
    if _rank >= _world_size:
        raise ValueError(f"Rank {_rank} >= WORLD_SIZE {_world_size}")

    # Optional: set backend-specific config
    if backend == "mpi":
        from .backend.mpi import init
        init(**kwargs)
    elif backend == "ucx":
        from .backend.ucx import init
        init(**kwargs)
    else:
        raise ValueError(f"Unsupported backend: {backend}")

def get_rank() -> int:
    return _rank

def get_world_size() -> int:
    return _world_size

def is_initialized() -> bool:
    return _rank is not None
```

> âœ… **No global state pollution** â€” uses module-level globals, but only for internal tracking.

---

#### 3. **`communication.py` â€“ Core Primitives**

```python
# tinygrad/distributed/communication.py
from typing import List, Any
import numpy as np

def all_reduce(tensor, op="sum"):
    """Reduce tensor across all ranks using specified op."""
    from .process_group import get_rank, get_world_size
    from .backend import get_backend

    backend = get_backend()
    if backend == "mpi":
        from .backend.mpi import all_reduce
        return all_reduce(tensor, op=op)
    elif backend == "ucx":
        from .backend.ucx import all_reduce
        return all_reduce(tensor, op=op)
    else:
        raise RuntimeError("No backend initialized")

def all_gather(tensor, dim=0):
    """Gather tensor from all ranks along dim."""
    from .process_group import get_rank, get_world_size
    from .backend import get_backend

    backend = get_backend()
    if backend == "mpi":
        from .backend.mpi import all_gather
        return all_gather(tensor, dim=dim)
    elif backend == "ucx":
        from .backend.ucx import all_gather
        return all_gather(tensor, dim=dim)
    else:
        raise RuntimeError("No backend initialized")

def broadcast(tensor, src_rank=0):
    """Broadcast tensor from src_rank to all ranks."""
    from .process_group import get_rank, get_world_size
    from .backend import get_backend

    backend = get_backend()
    if backend == "mpi":
        from .backend.mpi import broadcast
        return broadcast(tensor, src_rank=src_rank)
    elif backend == "ucx":
        from .backend.ucx import broadcast
        return broadcast(tensor, src_rank=src_rank)
    else:
        raise RuntimeError("No backend initialized")

def send(tensor, dst_rank: int):
    """Send tensor to dst_rank."""
    from .process_group import get_rank
    from .backend import get_backend

    backend = get_backend()
    if backend == "mpi":
        from .backend.mpi import send
        return send(tensor, dst_rank=dst_rank)
    elif backend == "ucx":
        from .backend.ucx import send
        return send(tensor, dst_rank=dst_rank)
    else:
        raise RuntimeError("No backend initialized")

def recv(tensor, src_rank: int):
    """Receive tensor from src_rank."""
    from .process_group import get_rank
    from .backend import get_backend

    backend = get_backend()
    if backend == "mpi":
        from .backend.mpi import recv
        return recv(tensor, src_rank=src_rank)
    elif backend == "ucx":
        from .backend.ucx import recv
        return recv(tensor, src_rank=src_rank)
    else:
        raise RuntimeError("No backend initialized")
```

> âœ… **All operations are non-blocking** in backend (e.g., `MPI_Isend`, `MPI_Irecv`).

---

#### 4. **`backend/mpi.py` â€“ MPI Backend (Optional)**

```python
# tinygrad/distributed/backend/mpi.py
import mpi4py.MPI as MPI
import numpy as np

def init():
    global comm
    comm = MPI.COMM_WORLD

def all_reduce(tensor, op="sum"):
    from tinygrad.tensor import Tensor
    if isinstance(tensor, Tensor):
        tensor = tensor.data
    if isinstance(tensor, np.ndarray):
        result = np.empty_like(tensor)
        comm.Allreduce(tensor, result, op=MPI.SUM)
        return Tensor(result)
    raise TypeError("Unsupported type")

def all_gather(tensor, dim=0):
    from tinygrad.tensor import Tensor
    if isinstance(tensor, Tensor):
        tensor = tensor.data
    if isinstance(tensor, np.ndarray):
        # Gather along dim
        result = np.empty_like(tensor)
        comm.Allgather(tensor, result)
        return Tensor(result)
    raise TypeError("Unsupported type")

def broadcast(tensor, src_rank=0):
    from tinygrad.tensor import Tensor
    if isinstance(tensor, Tensor):
        tensor = tensor.data
    if isinstance(tensor, np.ndarray):
        result = np.empty_like(tensor)
        comm.Bcast(tensor, root=src_rank)
        return Tensor(result)
    raise TypeError("Unsupported type")

def send(tensor, dst_rank):
    from tinygrad.tensor import Tensor
    if isinstance(tensor, Tensor):
        tensor = tensor.data
    if isinstance(tensor, np.ndarray):
        comm.Send(tensor, dest=dst_rank)
    else:
        raise TypeError("Unsupported type")

def recv(tensor, src_rank):
    from tinygrad.tensor import Tensor
    if isinstance(tensor, Tensor):
        tensor = tensor.data
    if isinstance(tensor, np.ndarray):
        comm.Recv(tensor, source=src_rank)
    else:
        raise TypeError("Unsupported type")
```

> âœ… **Uses `mpi4py`** â€” optional dependency (`pip install mpi4py`).

---

#### 5. **`backend/ucx.py` â€“ UCX Backend (Optional)**

```python
# tinygrad/distributed/backend/ucx.py
# Requires: pip install ucx-py
import ucx_api as ucx
import numpy as np

def init():
    # Initialize UCX
    ucx.init()

def all_reduce(tensor, op="sum"):
    # Implementation using UCX primitives
    # (simplified for brevity)
    pass

# ... similar for other ops
```

> âœ… **UCX is faster than MPI** for GPU-to-GPU, but requires `ucx-py`.

---

### âœ… **Key Technical Details**
- **No global state** â€” only module-level globals.
- **All operations are explicit** â€” no hidden state.
- **Backends are optional** â€” users install `mpi4py` or `ucx-py` only if needed.
- **No PyTorch** â€” pure Python + C extensions (CUDA/OpenCL already used).
- **Non-blocking** communication via `MPI_Isend`, `MPI_Irecv`.

---

## ðŸ”¹ PR #2: `tinygrad.distributed.parallel` â€“ ZeRO-2 Implementation

### ðŸŽ¯ **Objective**
Implement **ZeRO-2** (Zero Redundancy Optimizer, Stage 2) **without modifying core autograd**.

---

### ðŸ§± **Implementation Details**

#### 1. **`ZeroOptimizer` Class**

```python
# tinygrad/distributed/parallel.py
from typing import List, Dict, Any
from tinygrad.tensor import Tensor
from tinygrad.nn import Optimizer

class ZeroOptimizer:
    def __init__(self, model, optimizer, device):
        self.model = model
        self.optimizer = optimizer
        self.device = device
        self.param_to_rank = {}
        self.local_states = {}
        self.global_states = {}
        self.world_size = dist.get_world_size()
        self.rank = dist.get_rank()

        # Shard optimizer states
        for i, p in enumerate(model.parameters()):
            rank = i % self.world_size
            if rank == self.rank:
                self.local_states[p] = self._init_state(p)
            self.param_to_rank[p] = rank

    def _init_state(self, p: Tensor) -> Dict[str, Tensor]:
        """Initialize optimizer state (e.g., Adam) for a parameter."""
        state = {}
        if isinstance(self.optimizer, Optimizer):
            # Use optimizer's state init logic
            # e.g., exp_avg, exp_avg_sq
            pass
        return state

    def step(self):
        """Update local optimizer states."""
        for p in self.model.parameters():
            if self.param_to_rank[p] == self.rank:
                # Update local state
                self.optimizer.step(p)

        # All-reduce gradients globally
        for p in self.model.parameters():
            if p.grad is not None:
                dist.all_reduce(p.grad, op="sum")

    def state_dict(self):
        """Return local state dict."""
        return {p: self.local_states[p] for p in self.local_states}

    def load_state_dict(self, state_dict):
        """Load local state."""
        for p, state in state_dict.items():
            if p in self.local_states:
                self.local_states[p] = state
```

> âœ… **No global optimizer state** â€” only local per-rank.

---

### âœ… **Key Technical Details**
- **Per-parameter sharding** â€” each rank owns a subset of optimizer states.
- **All-reduce gradients** â€” ensures global consistency.
- **No global state** â€” avoids memory bloat.
- **Composable** â€” wraps any optimizer (`Adam`, `SGD`).

---

## ðŸ”¹ PR #3: `tinygrad.distributed.checkpoint` â€“ Activation Recomputation

### ðŸŽ¯ **Objective**
Implement **activation recomputation** (gradient checkpointing) **without modifying autograd engine**.

---

### ðŸ§± **Implementation Details**

#### 1. **`checkpoint` Context Manager**

```python
# tinygrad/distributed/checkpoint.py
from contextlib import contextmanager
from tinygrad.tensor import Tensor
from tinygrad.nn import Module

@contextmanager
def checkpoint(func):
    """Context manager to checkpoint a function."""
    saved_tensors = []
    saved_names = []

    def wrapper(*args, **kwargs):
        with no_grad():
            out = func(*args, **kwargs)
        saved_tensors.append(out)
        saved_names.append(f"out_{len(saved_tensors)}")
        return out

    # Store original function
    wrapper.original_func = func
    wrapper.saved_tensors = saved_tensors
    wrapper.saved_names = saved_names

    yield wrapper

    # After context exit, recompute if needed
    # (handled by autograd engine via `recompute`)
```

#### 2. **Integrate with `tinygrad.nn.Module`**

```python
# In a layer (e.g., Attention)
class Attention(Module):
    def __init__(self, ...):
        self.qkv = Linear(...)
        self.proj = Linear(...)

    def forward(self, x):
        with checkpoint(self._forward_with_recompute):
            return self._forward_with_recompute(x)

    def _forward_with_recompute(self, x):
        q, k, v = self.qkv(x).split(3, dim=-1)
        attn = q @ k.transpose(-2, -1) / sqrt(d)
        attn = attn.softmax(-1)
        out = attn @ v
        return self.proj(out)
```

> âœ… **No change to autograd engine** â€” just wrap forward pass.

---

### âœ… **Key Technical Details**
- **Only saves outputs** (not inputs).
- **Recomputes only when needed** (during backward).
- **Uses `no_grad()`** during recomputation.
- **No memory spikes**.

---